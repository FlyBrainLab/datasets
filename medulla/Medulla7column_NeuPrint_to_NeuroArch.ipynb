{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading NeuroArch Database with Medulla 7 Column Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial provides code to load NeuroArch database with Medulla 7 Column Dataset. Requirement before running the notebook:\n",
    "- Installed [NeuroArch](https://github.com/fruitflybrain/neuroarch), [OrientDB Community Version](https://www.orientdb.org/download), and [pyorient](https://github.com/fruitflybrain/pyorient). The [NeuroNLP Docker image](https://hub.docker.com/r/fruitflybrain/neuronlp) and [FlyBrainLab Docker image](https://hub.docker.com/r/fruitflybrain/fbl) all have a copy of the software requirement ready.\n",
    "- Download the [Neuprint database dump for the Medulla 7 Column dataset](https://github.com/connectome-neuprint/neuPrint/raw/master/fib25_neo4j_inputs.zip).\n",
    "- Have 1GB free disk space (for Neuprint dump and NeuroArch database).\n",
    "\n",
    "A backup of the database created by this notebook can be downloaded [here](https://drive.google.com/file/d/1YTkoSwHeh0bIu8O-2Ek98rY0udoCb3MA/view?usp=sharing). To restore it in OrientDB, run\n",
    "```\n",
    "/path/to/orientdb/bin/console.sh \"create database plocal:../databases/medulla admin admin; restore database /path/to/medulla7column_2b6d75f1c_na_v1.0_backup.zip\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import subprocess\n",
    "import csv\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import h5py\n",
    "\n",
    "import neuroarch.models as models\n",
    "import neuroarch.na as na"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Neuron and Synapse Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(chunk):\n",
    "    status = np.nonzero(np.array([i == 'Traced' for i in chunk['status:string'].values]))[0]\n",
    "    used = chunk.iloc[status]\n",
    "    neurons = []\n",
    "\n",
    "    for i, row in used.iterrows():\n",
    "        li = [row['bodyId:long'], row['pre:int'], row['post:int'], row['status:string'],\\\n",
    "              row['statusLabel:string'], int(row['cropped:boolean']) if not np.isnan(row['cropped:boolean']) else row['cropped:boolean'], row['instance:string'], \\\n",
    "              row['type:string']]\n",
    "        neurons.append(li)\n",
    "    return neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunksize = 100000\n",
    "\n",
    "with open('neurons.csv', 'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['bodyID','pre','post','status','statusLabel','cropped','instance','type'])\n",
    "    df = pd.read_csv('fib25/Neuprint_Neurons_fib25.csv')\n",
    "    neurons = process(df)\n",
    "    writer.writerows(neurons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neurons = pd.read_csv('neurons.csv')\n",
    "used = []\n",
    "swc_dir = 'swc'\n",
    "for i, row in neurons.iterrows():\n",
    "    if os.path.exists('{}/{}.swc'.format(swc_dir, row['bodyID'])):\n",
    "        if isinstance(row['instance'], str):\n",
    "            if row['instance'] in ['glia', 'cell body']:\n",
    "                continue\n",
    "            used.append(i)\n",
    "        elif isinstance(row['type'], str):\n",
    "            used.append(i)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traced_neuron_id = neurons.iloc[used]['bodyID'].to_numpy()\n",
    "        \n",
    "chunksize = 1000000\n",
    "pre_syn = np.empty((int(1e7),3), np.int64)\n",
    "post_syn = np.empty((int(1e7),3), np.int64)\n",
    "\n",
    "pre_count = 0\n",
    "post_count = 0\n",
    "count = 0\n",
    "for chunk in pd.read_csv('fib25/Neuprint_SynapseSet_to_Synapses_fib25.csv', chunksize=chunksize):\n",
    "    ids = chunk[':START_ID']\n",
    "    pre_site = np.array([[n, int(i.split('_')[0]), int(i.split('_')[1])] \\\n",
    "                         for n,i in enumerate(ids) if i.split('_')[2] == 'pre'])\n",
    "    post_site = np.array([[n, int(i.split('_')[0]), int(i.split('_')[1])] \\\n",
    "                          for n,i in enumerate(ids) if i.split('_')[2] == 'post'])\n",
    "    pre_site_known = pre_site[np.logical_and(\n",
    "                              np.isin(pre_site[:,1], traced_neuron_id),\n",
    "                              np.isin(pre_site[:,2], traced_neuron_id)),0]\n",
    "    post_site_known = post_site[np.logical_and(\n",
    "                                np.isin(post_site[:,1], traced_neuron_id),\n",
    "                                np.isin(post_site[:,2], traced_neuron_id)),0]\n",
    "    retrieved_pre_site = chunk.iloc[pre_site_known]\n",
    "    pre_site = np.array([[row[':END_ID(Syn-ID)'], int(row[':START_ID'].split('_')[0]), int(row[':START_ID'].split('_')[1])] \\\n",
    "                         for i, row in retrieved_pre_site.iterrows()])\n",
    "    retrieved_post_site = chunk.iloc[post_site_known]\n",
    "    post_site = np.array([[row[':END_ID(Syn-ID)'], int(row[':START_ID'].split('_')[0]), int(row[':START_ID'].split('_')[1])] \\\n",
    "                         for i, row in retrieved_post_site.iterrows()])\n",
    "    if pre_site.size:\n",
    "        pre_syn[pre_count:pre_count+pre_site.shape[0], :] = pre_site\n",
    "        pre_count += pre_site.shape[0]\n",
    "    if post_site.size:\n",
    "        post_syn[post_count:post_count+post_site.shape[0], :] = post_site\n",
    "        post_count += post_site.shape[0]\n",
    "    count += chunksize\n",
    "    print(count, pre_count, post_count)\n",
    "\n",
    "pre_syn = pre_syn[:pre_count,:]\n",
    "post_syn = post_syn[:post_count,:]\n",
    "\n",
    "ind = np.argsort(pre_syn[:,0])\n",
    "pre_syn_sorted = pre_syn[ind, :]\n",
    "ind = np.argsort(post_syn[:,0])\n",
    "post_syn_sorted = post_syn[ind, :]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract synapse (pre-site) to synapse (post-site) connection\n",
    "# use only the post synaptic site to get all the synapses because one presynaptic site can have multiple postsynaptic sites\n",
    "post_syn_index = post_syn_sorted[:,0].copy()\n",
    "\n",
    "df = pd.read_csv('fib25/Neuprint_Synapse_Connections_fib25.csv')\n",
    "post_ids = df[':END_ID(Syn-ID)']\n",
    "used = np.where(post_ids.isin(post_syn_index).to_numpy())[0]\n",
    "connections = df.iloc[used].to_numpy()\n",
    "ind = np.argsort(connections[:,1])\n",
    "connections = connections[ind, :]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract synapse details\n",
    "# with h5py.File('syn_pre_post_sorted_by_synapse_id.h5', 'r') as f:\n",
    "#     pre_syn_sorted = f['pre_syn_sorted'][:]\n",
    "#     post_syn_sorted = f['post_syn_sorted'][:]\n",
    "chunksize = 100000\n",
    "\n",
    "pre_syn_index = list(set(pre_syn_sorted[:,0].copy()))\n",
    "pre_syn_index.extend(list(post_syn_sorted[:,0].copy()))\n",
    "syn_index = np.array(sorted(pre_syn_index))\n",
    "del pre_syn_index#, pre_syn_sorted, post_syn_sorted\n",
    "\n",
    "synapse_array = np.empty((len(syn_index), 6), np.int64)\n",
    "\n",
    "synapse_count = 0\n",
    "count = 0\n",
    "\n",
    "for chunk in pd.read_csv('fib25/Neuprint_Synapses_fib25.csv', chunksize=chunksize):\n",
    "    ids = chunk[':ID(Syn-ID)']\n",
    "    \n",
    "    start_id = ids.iloc[0]\n",
    "    stop_id = ids.iloc[-1]\n",
    "    pre_start = np.searchsorted(syn_index, start_id, side='left')\n",
    "    pre_end = np.searchsorted(syn_index, stop_id, side='right')\n",
    "    if pre_start >= len(syn_index):\n",
    "        pre_index = []\n",
    "    else:\n",
    "        if pre_end >= len(syn_index):\n",
    "            pre_index = syn_index[pre_start:pre_end] #same as syn_index[pre_start:]\n",
    "        else:\n",
    "            pre_index = syn_index[pre_start:pre_end]\n",
    "    pre_used_synapse = chunk.loc[ids.isin(pre_index)]\n",
    "    li = np.empty((pre_index.size, 6), np.int64)\n",
    "    i = 0\n",
    "    for _, row in pre_used_synapse.iterrows():\n",
    "        location = eval(row['location:point{srid:9157}'].replace('x', \"'x'\").replace('y', \"'y'\").replace('z', \"'z'\"))\n",
    "        li[i,:] = [row[':ID(Syn-ID)'], # synpase id\n",
    "                     0 if row['type:string'] == 'pre' else 1, #synapse type\n",
    "                     int(row['confidence:float']*1000000), #confidence\n",
    "                     location['x'], location['y'], location['z']]\n",
    "        i += 1\n",
    "    synapse_array[synapse_count:synapse_count+pre_index.shape[0],:] = li\n",
    "    synapse_count += pre_index.shape[0]\n",
    "    count += chunksize\n",
    "    print(count, len(pre_used_synapse))\n",
    "synapse_array = synapse_array[:synapse_count,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reorder synapses\n",
    "\n",
    "synapse_connections = connections\n",
    "    \n",
    "ids = synapse_array[:,0]\n",
    "syn_id_dict = {j: i for i, j in enumerate(ids)}\n",
    "# ids = pre_syn_sorted[:,0]\n",
    "# pre_syn_id_dict = {j: i for i, j in enumerate(ids)} # map syn id to pre_syn_sorted\n",
    "ids = post_syn_sorted[:,0]\n",
    "post_syn_id_dict = {j: i for i, j in enumerate(ids)} # map syn id to post_syn_sorted\n",
    "\n",
    "synapse_dict = {}\n",
    "wrong_synapse = 0\n",
    "for i, pair in tqdm(enumerate(synapse_connections)):\n",
    "    pre_syn_id = pair[0]\n",
    "    post_syn_id = pair[1]\n",
    "    post_id = post_syn_id_dict[post_syn_id]\n",
    "    post_info = synapse_array[syn_id_dict[post_syn_id]]\n",
    "    post_neuron_id, pre_neuron_id = post_syn_sorted[post_id, 1:]\n",
    "\n",
    "    #if len(np.where((pre_syn_sorted == (pre_syn_id, pre_neuron_id, post_neuron_id)).all(axis=1))[0]) != 1:\n",
    "    #    print(pre_syn_id, post_syn_id)\n",
    "    # pre_id = pre_syn_id_dict[pre_syn_id]\n",
    "    pre_info = synapse_array[syn_id_dict[pre_syn_id]]\n",
    "\n",
    "    if pre_neuron_id not in synapse_dict:\n",
    "        synapse_dict[pre_neuron_id] = {}\n",
    "    pre_dict = synapse_dict[pre_neuron_id]\n",
    "    if post_neuron_id not in synapse_dict[pre_neuron_id]:\n",
    "        pre_dict[post_neuron_id] =  {'pre_synapse_ids': [],\n",
    "                                     'post_synapse_ids': [],\n",
    "                                     'pre_confidence': [],\n",
    "                                     'post_confidence': [],\n",
    "                                     'pre_x': [],\n",
    "                                     'pre_y': [],\n",
    "                                     'pre_z': [],\n",
    "                                     'post_x': [],\n",
    "                                     'post_y': [],\n",
    "                                     'post_z': [],\n",
    "                                     }\n",
    "    info_dict = pre_dict[post_neuron_id]\n",
    "    info_dict['pre_synapse_ids'].append(pre_syn_id)\n",
    "    info_dict['post_synapse_ids'].append(post_syn_id)\n",
    "    info_dict['pre_confidence'].append(pre_info[2])\n",
    "    info_dict['post_confidence'].append(post_info[2])\n",
    "    info_dict['pre_x'].append(pre_info[3])\n",
    "    info_dict['pre_y'].append(pre_info[4])\n",
    "    info_dict['pre_z'].append(pre_info[5])\n",
    "    info_dict['post_x'].append(post_info[3])\n",
    "    info_dict['post_y'].append(post_info[4])\n",
    "    info_dict['post_z'].append(post_info[5])\n",
    "\n",
    "with open('synapses.csv', 'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['pre_id','post_id','N','pre_confidence','post_confidence',\\\n",
    "                     'pre_x','pre_y','pre_z','post_x','post_y','post_z'])\n",
    "    for pre, k in tqdm(synapse_dict.items()):\n",
    "        for post, v in k.items():\n",
    "            writer.writerow([pre, post, len(v['pre_x']), str(v['pre_confidence']), \\\n",
    "                             str(v['post_confidence']), str(v['pre_x']), str(v['pre_y']), str(v['pre_z']), \\\n",
    "                             str(v['post_x']), str(v['post_y']), str(v['post_z'])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data to NeuroArch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "medulla = na.NeuroArch('localhost', 'medulla', mode = 'o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "species = medulla.add_species('Drosophila melanogaster', stage = 'adult',\n",
    "                                sex = 'female',\n",
    "                                synonyms = ['fruit fly', 'common fruit fly', 'vinegar fly'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = '2b6d75f1c'\n",
    "datasource = medulla.add_DataSource('Medulla7column', version = version,\n",
    "                                      url = 'https://www.janelia.org/project-team/flyem/research/previous-connectomes-analyzed/seven-column-connectome-fib-sem',\n",
    "                                      description = 'data obtained from https://github.com/connectome-neuprint/neuPrint/blob/922a107df827a2fedd671438595603c4d15eafa7/fib25_neo4j_inputs.zip; neuron skeleton from https://github.com/janelia-flyem/ConnectomeHackathon2015',\n",
    "                                      species = species)\n",
    "medulla.default_DataSource = datasource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "medulla.add_Neuropil('MED(L)', synonyms = ['left medulla'])\n",
    "\n",
    "for i in range(1, 11):\n",
    "    medulla.add_Subregion('MED-M{}(L)'.format(i),\\\n",
    "                          synonyms = ['left medulla M{} stratum'.format(i), 'left medulla stratum M{}'.format(i)],\n",
    "                          neuropil = 'MED(L)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron_list = pd.read_csv('neurons.csv')\n",
    "swc_dir = 'swc'\n",
    "uname_dict = {}\n",
    "\n",
    "for i, row in tqdm(neuron_list.iterrows()):\n",
    "    if isinstance(row['instance'], str):\n",
    "        if row['instance'] in ['glia', 'cell body']:\n",
    "            continue\n",
    "    elif isinstance(row['type'], str):\n",
    "        pass\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "    bodyID = row['bodyID']\n",
    "    cell_type = row['type']\n",
    "    name = row['instance']\n",
    "    \n",
    "    if not os.path.exists('{}/{}.swc'.format(swc_dir, bodyID)):\n",
    "        continue\n",
    "    \n",
    "    if not isinstance(name, str):\n",
    "        if isinstance(cell_type, str):\n",
    "            name = '{}-{}'.format(cell_type, bodyID)\n",
    "        else:\n",
    "            cell_type = 'unknown'\n",
    "            name = 'unknown-{}'.format(bodyID)\n",
    "    else:\n",
    "        if not isinstance(cell_type, str):\n",
    "            if name.split('-')[0] == 'tan':\n",
    "                cell_type = 'tangential'\n",
    "                if name not in uname_dict:\n",
    "                    uname_dict[name] = 0\n",
    "                uname_dict[name] += 1\n",
    "                name = '{}-{}'.format(name, uname_dict[name])\n",
    "            elif name.split('-')[0] == 'out':\n",
    "                cell_type = 'output'\n",
    "            elif name in ['Tm23/24', 'Tm23/24-F', 'Dm8-out', 'TmY16?']:\n",
    "                cell_type = name\n",
    "                if name not in uname_dict:\n",
    "                    uname_dict[name] = 0\n",
    "                uname_dict[name] += 1\n",
    "                name = '{}-{}'.format(name, uname_dict[name])\n",
    "            else:\n",
    "                cell_type = name.split('-')[0]\n",
    "        else:\n",
    "            if name in ['Tm23/24', 'Tm23/24-F', 'Dm8-out', 'TmY16?']:\n",
    "                if name not in uname_dict:\n",
    "                    uname_dict[name] = 0\n",
    "                uname_dict[name] += 1\n",
    "                name = '{}-{}'.format(name, uname_dict[name])\n",
    "    if ' home' in name:\n",
    "        name.replace(' home', '-home')\n",
    "                \n",
    "    info = {}\n",
    "    \n",
    "    medulla.add_Neuron(name, # uname\n",
    "                         cell_type, # name\n",
    "                         referenceId = str(bodyID), #referenceId\n",
    "                         info = info if len(info) else None,\n",
    "                         morphology = {'type': 'swc', 'filename': '{}/{}.swc'.format(swc_dir, bodyID), 'scale': 0.001*10},)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neurons = medulla.sql_query('select from Neuron').nodes_as_objs\n",
    "# set the cache so there is no need for database access.\n",
    "for neuron in neurons:\n",
    "    medulla.set('Neuron', neuron.uname, neuron, medulla.default_DataSource)\n",
    "neuron_ref_to_obj = {int(neuron.referenceId): neuron for neuron in neurons}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synapse_df = pd.read_csv('synapses.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in tqdm(synapse_df.iterrows()):\n",
    "    pre_neuron = neuron_ref_to_obj[row['pre_id']]\n",
    "    post_neuron = neuron_ref_to_obj[row['post_id']]\n",
    "\n",
    "    pre_conf = np.array(eval(row['pre_confidence']))/1e6\n",
    "    post_conf = np.array(eval(row['post_confidence']))/1e6\n",
    "    NHP = np.sum(np.logical_and(post_conf>=0.7, pre_conf>=0.7))\n",
    "\n",
    "    content = {'type': 'swc'}\n",
    "    content['x'] = [round(i, 3) for i in (np.array(eval(row['pre_x'])+eval(row['post_x']))/1000.*10).tolist()]\n",
    "    content['y'] = [round(i, 3) for i in (np.array(eval(row['pre_y'])+eval(row['post_y']))/1000.*10).tolist()]\n",
    "    content['z'] = [round(i, 3) for i in (np.array(eval(row['pre_z'])+eval(row['post_z']))/1000.*10).tolist()]\n",
    "    content['r'] = [0]*len(content['x'])\n",
    "    content['parent'] = [-1]*(len(content['x'])//2) + [i+1 for i in range(len(content['x'])//2)]\n",
    "    content['identifier'] = [7]*(len(content['x'])//2) + [8]*(len(content['x'])//2)\n",
    "    content['sample'] = [i+1 for i in range(len(content['x']))]\n",
    "    content['confidence'] = [round(i, 3) for i in pre_conf.tolist()] + [round(i, 3) for i in post_conf.tolist()]\n",
    "    \n",
    "    medulla.add_Synapse(pre_neuron, post_neuron, N = row['N'], NHP = NHP,\n",
    "                          morphology = content)\n",
    "                          #arborization = arborization)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure out Arborization Data from Loaded Synapse Positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strata_arborization(z):\n",
    "    sep = np.array([20, 28, 35, 38, 42, 44, 55, 58, 70])\n",
    "    return Counter(np.digitize(z, sep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "neuron_dict = {}\n",
    "\n",
    "for neuron in neurons:\n",
    "    neuron_dict[neuron.uname] = {'axons': Counter(), 'dendrites': Counter(), 'obj': neuron} \n",
    "    \n",
    "for neuron in tqdm(neurons):\n",
    "    outgoing_synapses = medulla.sql_query(\"\"\"select expand(out('SendsTo')) from {}\"\"\".format(neuron._id)).nodes_as_objs\n",
    "    for synapse in outgoing_synapses:\n",
    "        morphology = [n for n in synapse.out('HasData') if isinstance(n, models.MorphologyData)][0]\n",
    "        arborization = []\n",
    "        arborization.append({'type': 'neuropil',\n",
    "                             'synapses': {'MED(L)': len(morphology.x)}})\n",
    "        s = strata_arborization(morphology.z[:(len(morphology.z)//2)])\n",
    "        arborization.append({'type': 'subregion',\n",
    "                             'synapses': {'MED-M{}(L)'.format(k+1): v for k, v in s.items()}})\n",
    "        neuron_dict[neuron.uname]['axons'] += s\n",
    "        neuron_dict[synapse.out('SendsTo')[0].uname]['dendrites'] += s\n",
    "        medulla.add_synapse_arborization(synapse, arborization)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for neuron in tqdm(neurons):\n",
    "    arborization = []\n",
    "    arborization.append({'type': 'neuropil',\n",
    "                         'dendrites': {'MED(L)': sum(neuron_dict[neuron.uname]['dendrites'].values())},\n",
    "                         'axons': {'MED(L)': sum(neuron_dict[neuron.uname]['axons'].values())}})\n",
    "    arborization.append({'type': 'subregion',\n",
    "                         'dendrites': {'MED-M{}(L)'.format(k+1): v for k, v in neuron_dict[neuron.uname]['dendrites'].items()},\n",
    "                         'axons': {'MED-M{}(L)'.format(k+1): v for k, v in neuron_dict[neuron.uname]['axons'].items()}})\n",
    "    medulla.add_neuron_arborization(neuron, arborization)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
