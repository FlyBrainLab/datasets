{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63f35a6e",
   "metadata": {},
   "source": [
    "This tutorial provides code to load NeuroArch database with MANC Dataset v1.0. Requirement before running the notebook:\n",
    "- Installed [NeuroArch](https://github.com/fruitflybrain/neuroarch), [OrientDB Community Version](https://www.orientdb.org/download) version 3.1.x, and [pyorient](https://github.com/fruitflybrain/pyorient). The [NeuroNLP Docker image](https://hub.docker.com/r/fruitflybrain/neuronlp) and [FlyBrainLab Docker image](https://hub.docker.com/r/fruitflybrain/fbl) all have a copy of the software requirement ready.\n",
    "- Installed [PyMeshLab](https://pypi.org/project/pymeshlab/).\n",
    "- Installed [neuprint-python](https://github.com/connectome-neuprint/neuprint-python).\n",
    "- Download the [Neuprint database dump for the MANC dataset v1.0](https://storage.googleapis.com/flyem-manc-exports/v1.0/neuprint_manc_v1.0/neuprint_manc_v1.0_csv.tar.gz).\n",
    "- Have the [token](https://connectome-neuprint.github.io/neuprint-python/docs/client.html#neuprint.client.Client) for Neuprint HTTP access ready.\n",
    "- Have more than 30 GB free disk space (for Neuprint dump and NeuroArch database).\n",
    "\n",
    "A backup of the database created by this notebook can be downloaded [here](https://drive.google.com/file/d/15MgSmFMFl_vUtS32rVpb0E7HKpJAQe8v/view?usp=drive_link). To restore it in OrientDB, run\n",
    "```\n",
    "/path/to/orientdb/bin/console.sh \"create database plocal:../databases/manc admin admin; restore database /path/to/manc1.0_na_v1.0.0_backup.zip\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212298f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import subprocess\n",
    "import csv\n",
    "import json\n",
    "import warnings\n",
    "from requests import HTTPError\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# from neuprint import Client\n",
    "from tqdm import tqdm\n",
    "import h5py\n",
    "import pymeshlab as ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae9977e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define region of interests (ROIs)\n",
    "all_rois = {}\n",
    "with open(\"neuprint_manc_v1.0_csv/all_ROIs.txt\") as f:\n",
    "    for line in f:\n",
    "        entry = {'System': 'VNC', 'Neuropil':line.strip()}\n",
    "        all_rois[line.strip()]= entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f792ef09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(chunk):\n",
    "    #status = np.nonzero(np.array([i == 'Traced' for i in chunk['status:string'].values]))[0]\n",
    "    status = np.nonzero((chunk['upstream:int']+chunk['downstream:int']!=0).to_numpy())[0]\n",
    "    used = chunk.iloc[status]\n",
    "    #used = chunk\n",
    "    neurons = []\n",
    "\n",
    "    for i, row in used.iterrows():\n",
    "        neuropil_list = []\n",
    "        kk = json.loads(row['roiInfo:string'])\n",
    "        for k, v in kk.items():\n",
    "            if k == \"None\": continue\n",
    "            region = all_rois[k]\n",
    "            neuropil_list.append('{}:{}:{}'.format(\n",
    "                region['Neuropil'], v.get('upstream', 0), v.get('downstream', 0)))\n",
    "\n",
    "        neuropil_list = ';'.join(neuropil_list)\n",
    "        \"instance:string\", \"type:string\", \"systematicType:string\", \"hemilineage:string\", \"somaSide:string\", \"rootSide:string\", \"class:string\", \"subclass:string\",  \"entryNerve:string\", \"exitNerve:string\", \"rootSide:string\", \"somaNeuromere:string\", \"somaLocation:point{srid:9157}\", \"predictedNt:string\", \"predictedNtProb:float\", \"modality:string\", \"transmission:string\"\n",
    "        li = [row['bodyId:long'],\n",
    "              row['upstream:int'],\n",
    "              row['downstream:int'],\n",
    "              row['status:string'],\n",
    "              row['statusLabel:string'],\n",
    "              int(row['cropped:boolean']) if not np.isnan(row['cropped:boolean']) else row['cropped:boolean'],\n",
    "              row['instance:string'], \n",
    "              row['type:string'],\n",
    "              row[\"systematicType:string\"],\n",
    "              row[\"hemilineage:string\"],\n",
    "              row[\"somaSide:string\"],\n",
    "              row[\"rootSide:string\"],\n",
    "              row[\"class:string\"],\n",
    "              row[\"subclass:string\"],\n",
    "              row[\"entryNerve:string\"],\n",
    "              row[\"exitNerve:string\"],\n",
    "              row[\"rootSide:string\"],\n",
    "              row[\"somaNeuromere:string\"],\n",
    "              row[\"somaLocation:point{srid:9157}\"],\n",
    "              row[\"predictedNt:string\"],\n",
    "              row[\"predictedNtProb:float\"],\n",
    "              row[\"modality:string\"],\n",
    "              row[\"transmission:string\"],\n",
    "              row['size:long'],\n",
    "              neuropil_list]\n",
    "        neurons.append(li)\n",
    "    return neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf78ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunksize = 100000\n",
    "\n",
    "with open('neurons_all.csv', 'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['bodyID','pre','post','status','statusLabel','cropped',\n",
    "                     'instance','type', 'systematicType', 'hemilineage', 'somaSide', 'rootSide',\n",
    "                     'class','subclass', 'entryNerve', 'exitNerve', 'rootSide', 'somaNeuromere',\n",
    "                     'somaLocation', 'predictedNt', 'predictedNtProb', 'modality', 'transmission',\n",
    "                     'size','neuropils'])\n",
    "    for chunk in tqdm(pd.read_csv('neuprint_manc_v1.0_csv/Neuprint_Neurons_manc_v1.csv', chunksize=chunksize)):\n",
    "        neurons = process(chunk)\n",
    "        writer.writerows(neurons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf94c36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "neurons = pd.read_csv('neurons_all.csv')\n",
    "        \n",
    "traced_neuron_id = neurons['bodyID'].to_numpy()\n",
    "        \n",
    "chunksize = 1000000\n",
    "pre_syn = np.empty((int(2e8),3), np.int64)\n",
    "post_syn = np.empty((int(2e8),3), np.int64)\n",
    "\n",
    "pre_count = 0\n",
    "post_count = 0\n",
    "count = 0\n",
    "for chunk in pd.read_csv('neuprint_manc_v1.0_csv//Neuprint_SynapseSet_to_Synapses_manc_v1.csv', chunksize=chunksize):\n",
    "    ids = chunk[':START_ID(SynSet-ID)']\n",
    "    pre_site = np.array([[n, int(i.split('_')[0]), int(i.split('_')[1])] \\\n",
    "                         for n,i in enumerate(ids) if i.split('_')[2] == 'pre'])\n",
    "    post_site = np.array([[n, int(i.split('_')[0]), int(i.split('_')[1])] \\\n",
    "                          for n,i in enumerate(ids) if i.split('_')[2] == 'post'])\n",
    "    pre_site_known = pre_site[:,0]\n",
    "    post_site_known = post_site[:,0]\n",
    "    retrieved_pre_site = chunk.iloc[pre_site_known]\n",
    "    pre_site = np.array([[row[':END_ID(Syn-ID)'], int(row[':START_ID(SynSet-ID)'].split('_')[0]), int(row[':START_ID(SynSet-ID)'].split('_')[1])] \\\n",
    "                         for i, row in retrieved_pre_site.iterrows()])\n",
    "    retrieved_post_site = chunk.iloc[post_site_known]\n",
    "    post_site = np.array([[row[':END_ID(Syn-ID)'], int(row[':START_ID(SynSet-ID)'].split('_')[0]), int(row[':START_ID(SynSet-ID)'].split('_')[1])] \\\n",
    "                         for i, row in retrieved_post_site.iterrows()])\n",
    "    if pre_site.size:\n",
    "        pre_syn[pre_count:pre_count+pre_site.shape[0], :] = pre_site\n",
    "        pre_count += pre_site.shape[0]\n",
    "    if post_site.size:\n",
    "        post_syn[post_count:post_count+post_site.shape[0], :] = post_site\n",
    "        post_count += post_site.shape[0]\n",
    "    count += chunksize\n",
    "    print(count, pre_count, post_count)\n",
    "\n",
    "pre_syn = pre_syn[:pre_count,:]\n",
    "post_syn = post_syn[:post_count,:]\n",
    "\n",
    "ind = np.argsort(pre_syn[:,0])\n",
    "pre_syn_sorted = pre_syn[ind, :]\n",
    "ind = np.argsort(post_syn[:,0])\n",
    "post_syn_sorted = post_syn[ind, :]\n",
    "# with h5py.File('syn_pre_post_sorted_by_synapse_id.h5', 'w') as f:\n",
    "#     f['pre_syn_sorted'] = pre_syn_sorted\n",
    "#     f['post_syn_sorted'] = post_syn_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67cc5edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract synapse (pre-site) to synapse (post-site) connection\n",
    "# use only the post synaptic site to get all the synapses because one presynaptic site can have multiple postsynaptic sites\n",
    "# with h5py.File('syn_pre_post_sorted_by_synapse_id.h5', 'r') as f:\n",
    "#     post_syn_sorted = f['post_syn_sorted'][:]\n",
    "post_syn_index = post_syn_sorted[:,0].copy()\n",
    "\n",
    "df = pd.read_csv('neuprint_manc_v1.0_csv/Neuprint_Synapse_Connections_manc_v1.csv')\n",
    "post_ids = df[':END_ID(Syn-ID)']\n",
    "used = np.where(post_ids.isin(post_syn_index).to_numpy())[0]\n",
    "connections = df.iloc[used].to_numpy()\n",
    "ind = np.argsort(connections[:,1])\n",
    "connections = connections[ind, :]\n",
    "# with h5py.File('synapse_connections.h5', 'w') as f:\n",
    "#     f['synapse_connecions'] = connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae9ba34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract synapse details\n",
    "# with h5py.File('syn_pre_post_sorted_by_synapse_id.h5', 'r') as f:\n",
    "#     pre_syn_sorted = f['pre_syn_sorted'][:]\n",
    "#     post_syn_sorted = f['post_syn_sorted'][:]\n",
    "chunksize = 100000\n",
    "\n",
    "pre_syn_index = list(set(pre_syn_sorted[:,0].copy()))\n",
    "pre_syn_index.extend(list(post_syn_sorted[:,0].copy()))\n",
    "syn_index = np.array(sorted(pre_syn_index))\n",
    "del pre_syn_index#, pre_syn_sorted, post_syn_sorted\n",
    "\n",
    "synapse_array = np.empty((len(syn_index), 6), np.int64)\n",
    "synapse_innervate = np.empty((len(syn_index), 61), bool) # number of columns that are neuropil:bool\n",
    "\n",
    "synapse_count = 0\n",
    "count = 0\n",
    "\n",
    "for chunk in pd.read_csv('neuprint_manc_v1.0_csv/Neuprint_Synapses_manc_v1.csv', chunksize=chunksize):\n",
    "    ids = chunk[':ID(Syn-ID)']\n",
    "    \n",
    "    start_id = ids.iloc[0]\n",
    "    stop_id = ids.iloc[-1]\n",
    "    pre_start = np.searchsorted(syn_index, start_id, side='left')\n",
    "    pre_end = np.searchsorted(syn_index, stop_id, side='right')\n",
    "    if pre_start >= len(syn_index):\n",
    "        pre_index = []\n",
    "    else:\n",
    "        if pre_end >= len(syn_index):\n",
    "            pre_index = syn_index[pre_start:pre_end] #same as syn_index[pre_start:]\n",
    "        else:\n",
    "            pre_index = syn_index[pre_start:pre_end]\n",
    "    pre_used_synapse = chunk.loc[ids.isin(pre_index)]\n",
    "    li = np.empty((pre_index.size, 6), np.int64)\n",
    "    li1 = np.empty((pre_index.size, 61), bool) # number of columns that are neuropil:bool\n",
    "    i = 0\n",
    "    for _, row in pre_used_synapse.iterrows():\n",
    "        location = eval(row['location:point{srid:9157}'].replace('x', \"'x'\").replace('y', \"'y'\").replace('z', \"'z'\"))\n",
    "        li[i,:] = [row[':ID(Syn-ID)'], # synpase id\n",
    "                     0 if row['type:string'] == 'pre' else 1, #synapse type\n",
    "                     int(row['confidence:float']*1000000), #confidence\n",
    "                     location['x'], location['y'], location['z']]\n",
    "        li1[i,:] = ~np.isnan(np.asarray(row.values[9:], np.double))\n",
    "        i += 1\n",
    "    synapse_array[synapse_count:synapse_count+pre_index.shape[0],:] = li\n",
    "    synapse_innervate[synapse_count:synapse_count+pre_index.shape[0],:] = li1\n",
    "    synapse_count += pre_index.shape[0]\n",
    "    count += chunksize\n",
    "    print(count, len(pre_used_synapse))\n",
    "synapse_array = synapse_array[:synapse_count,:]\n",
    "synapse_innervate = synapse_innervate[:synapse_count,:]\n",
    "\n",
    "# with h5py.File('syn_used_details.h5', 'w') as f:\n",
    "#     f['synapse_array'] = synapse_array\n",
    "#     f['synapse_innervate'] = synapse_innervate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96fd9751",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reorder synapses\n",
    "\n",
    "# with h5py.File('syn_used_details.h5', 'r') as f:\n",
    "#     synapse_array = f['synapse_array'][:]\n",
    "#     synapse_innervate = f['synapse_innervate']\n",
    "\n",
    "# with h5py.File('synapse_connections.h5', 'r') as f:\n",
    "#     synapse_connections = f['synapse_connecions'][:]\n",
    " \n",
    "# with h5py.File('syn_pre_post_sorted_by_synapse_id.h5', 'r') as f:\n",
    "#     pre_syn_sorted = f['pre_syn_sorted'][:]\n",
    "#     post_syn_sorted = f['post_syn_sorted'][:]\n",
    "\n",
    "synapse_connections = connections\n",
    "    \n",
    "ids = synapse_array[:,0]\n",
    "syn_id_dict = {j: i for i, j in enumerate(ids)}\n",
    "# ids = pre_syn_sorted[:,0]\n",
    "# pre_syn_id_dict = {j: i for i, j in enumerate(ids)} # map syn id to pre_syn_sorted\n",
    "ids = post_syn_sorted[:,0]\n",
    "post_syn_id_dict = {j: i for i, j in enumerate(ids)} # map syn id to post_syn_sorted\n",
    "\n",
    "synapse_dict = {}\n",
    "wrong_synapse = 0\n",
    "for i, pair in tqdm(enumerate(synapse_connections)):\n",
    "    pre_syn_id = pair[0]\n",
    "    post_syn_id = pair[1]\n",
    "    post_id = post_syn_id_dict[post_syn_id]\n",
    "    post_info = synapse_array[syn_id_dict[post_syn_id]]\n",
    "    post_info1 = synapse_innervate[syn_id_dict[post_syn_id]]\n",
    "    post_neuron_id, pre_neuron_id = post_syn_sorted[post_id, 1:]\n",
    "\n",
    "    #if len(np.where((pre_syn_sorted == (pre_syn_id, pre_neuron_id, post_neuron_id)).all(axis=1))[0]) != 1:\n",
    "    #    print(pre_syn_id, post_syn_id)\n",
    "    # pre_id = pre_syn_id_dict[pre_syn_id]\n",
    "    pre_info = synapse_array[syn_id_dict[pre_syn_id]]\n",
    "    pre_info1 = synapse_innervate[syn_id_dict[pre_syn_id]]\n",
    "\n",
    "    if pre_neuron_id not in synapse_dict:\n",
    "        synapse_dict[pre_neuron_id] = {}\n",
    "    pre_dict = synapse_dict[pre_neuron_id]\n",
    "    if post_neuron_id not in synapse_dict[pre_neuron_id]:\n",
    "        pre_dict[post_neuron_id] =  {'pre_synapse_ids': [],\n",
    "                                     'post_synapse_ids': [],\n",
    "                                     'pre_confidence': [],\n",
    "                                     'post_confidence': [],\n",
    "                                     'pre_x': [],\n",
    "                                     'pre_y': [],\n",
    "                                     'pre_z': [],\n",
    "                                     'post_x': [],\n",
    "                                     'post_y': [],\n",
    "                                     'post_z': [],\n",
    "                                     'regions': np.zeros(61, np.int32)}\n",
    "    info_dict = pre_dict[post_neuron_id]\n",
    "    info_dict['pre_synapse_ids'].append(pre_syn_id)\n",
    "    info_dict['post_synapse_ids'].append(post_syn_id)\n",
    "    info_dict['pre_confidence'].append(pre_info[2])\n",
    "    info_dict['post_confidence'].append(post_info[2])\n",
    "    info_dict['pre_x'].append(pre_info[3])\n",
    "    info_dict['pre_y'].append(pre_info[4])\n",
    "    info_dict['pre_z'].append(pre_info[5])\n",
    "    info_dict['post_x'].append(post_info[3])\n",
    "    info_dict['post_y'].append(post_info[4])\n",
    "    info_dict['post_z'].append(post_info[5])\n",
    "    info_dict['regions'] += post_info1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e58cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk = pd.read_csv('neuprint_manc_v1.0_csv/Neuprint_Synapses_manc_v1.csv', chunksize=1).get_chunk()\n",
    "labels = [i.split(':')[0] for i in chunk.columns.to_list()]\n",
    "regions = labels[9:]\n",
    "\n",
    "with open('synapses_all.csv', 'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['pre_id','post_id','N','pre_confidence','post_confidence',\\\n",
    "                     'pre_x','pre_y','pre_z','post_x','post_y','post_z',\\\n",
    "                     'neuropils'])\n",
    "    for pre, k in tqdm(synapse_dict.items()):\n",
    "        for post, v in k.items():\n",
    "            reg = {regions[i]: v['regions'][i] for i in np.nonzero(v['regions'])[0]}\n",
    "            neuropil_list = []\n",
    "            for k, n in reg.items():\n",
    "                region = all_rois[k]\n",
    "                neuropil_list.append('{}:{}'.format(\n",
    "                    region['Neuropil'], n))\n",
    "\n",
    "\n",
    "            neuropil_list = ';'.join(neuropil_list)\n",
    "\n",
    "            writer.writerow([pre, post, len(v['pre_x']), str(v['pre_confidence']), \\\n",
    "                             str(v['post_confidence']), str(v['pre_x']), str(v['pre_y']), str(v['pre_z']), \\\n",
    "                             str(v['post_x']), str(v['post_y']), str(v['post_z']), \\\n",
    "                             neuropil_list])   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e9cbe9-eadc-4465-813b-b68ef8c491f1",
   "metadata": {},
   "source": [
    "## Loading NeuroArch Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d07ecc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import neuroarch.na as na"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a03bcbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "vnc = na.NeuroArch('manc', port = 2424, mode = 'o', version = \"1.0.0\",\n",
    "                   maintainer_name = \"\", maintainer_email = \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b952e01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "species = vnc.add_Species('Drosophila melanogaster', stage = 'adult',\n",
    "                            sex = 'male',\n",
    "                            synonyms = ['fruit fly', 'common fruit fly', 'vinegar fly'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3972e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "version = '1.0'\n",
    "datasource = vnc.add_DataSource('manc', version = version,\n",
    "                                      url = 'https://www.janelia.org/project-team/flyem/manc-connectome',\n",
    "                                      species = species)\n",
    "vnc.default_DataSource = datasource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d67936",
   "metadata": {},
   "outputs": [],
   "source": [
    "vnc.add_Subsystem('VNC', synonyms = ['ventral nerve cord'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20830002",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('filter_file_tmp.mlx', 'w') as f:\n",
    "    f.write(\"\"\"<!DOCTYPE FilterScript>\n",
    "<FilterScript>\n",
    " <filter name=\"Simplification: Quadric Edge Collapse Decimation\">\n",
    "  <Param type=\"RichInt\" value=\"60000\" name=\"TargetFaceNum\"/>\n",
    "  <Param type=\"RichFloat\" value=\"0.2\" name=\"TargetPerc\"/>\n",
    "  <Param type=\"RichFloat\" value=\"1\" name=\"QualityThr\"/>\n",
    "  <Param type=\"RichBool\" value=\"true\" name=\"PreserveBoundary\"/>\n",
    "  <Param type=\"RichFloat\" value=\"1\" name=\"BoundaryWeight\"/>\n",
    "  <Param type=\"RichBool\" value=\"true\" name=\"OptimalPlacement\"/>\n",
    "  <Param type=\"RichBool\" value=\"true\" name=\"PreserveNormal\"/>\n",
    " </filter>\n",
    "</FilterScript>\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6e2a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in all_rois.items():\n",
    "    if v['Neuropil'] is not None:\n",
    "        if isinstance(v['Neuropil'], list):\n",
    "            continue\n",
    "        ms = ml.MeshSet()\n",
    "        ms.load_new_mesh(\"corrected_roi/{}.obj\".format(k))\n",
    "        ms.load_filter_script('filter_file_tmp.mlx')\n",
    "        ms.apply_filter_script()\n",
    "        current_mesh = ms.current_mesh()\n",
    "        vnc.add_Neuropil(k,\n",
    "                               morphology = {'type': 'mesh',\n",
    "                                             \"vertices\": (current_mesh.vertex_matrix()).flatten().tolist(),\n",
    "                                             \"faces\": current_mesh.face_matrix().flatten().tolist()},\n",
    "                               subsystem = v['System'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbebcb8-2a79-4be3-a36d-8fa6fff3b6a6",
   "metadata": {},
   "source": [
    "#### Load Neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472f7299",
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron_list = pd.read_csv('neurons_all.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce4d5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_swc = [int(n.split('/')[1].split('.')[0]) for n in glob.glob('swc/*.swc')]\n",
    "has_swc = neuron_list['bodyID'].isin(all_swc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97cb9bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "swc_dir = 'swc'\n",
    "uname_dict = {}\n",
    "segment_ids = set()\n",
    "added = 0\n",
    "unadded = []\n",
    "to_combine = []\n",
    "loaded_to_segment = []\n",
    "\n",
    "neurotransmitter_translation = {'acetylcholine': 'acetylcholine',\n",
    "                                'gaba': 'GABA',\n",
    "                                'glutamate': 'glutamate'}\n",
    "x = 'x'\n",
    "y = 'y'\n",
    "z = 'z'\n",
    "\n",
    "choose = ((neuron_list['status']=='Traced') & (neuron_list['statusLabel']!='Leaves')) | (~neuron_list['instance'].isna())\n",
    "\n",
    "for i, row in tqdm(neuron_list[choose].iterrows()):\n",
    "    bodyID = row['bodyID']\n",
    "#     cell_type = row['type']\n",
    "#     name = row['instance']\n",
    "    name = None\n",
    "    cell_type = None\n",
    "    \n",
    "    segment = False\n",
    "    \n",
    "    info = {}\n",
    "    neurotransmitter = None\n",
    "    \n",
    "    if not isinstance(row['instance'], str) or row['instance'] == 'TBD':\n",
    "        if isinstance(row['systematicType'], str):\n",
    "            if isinstance(row['somaNeuromere'], str):\n",
    "                name = '{}_{}'.format(row['systematicType'], row['somaNeuromere'])\n",
    "            elif isinstance(row['entryNerve'], str):\n",
    "                name = '{}_{}'.format(row['systematicType'], row['entryNerve'].split('_')[0])\n",
    "            elif isinstance(row['exitNerve'], str):\n",
    "                name = '{}_{}'.format(row['systematicType'], row['exitNerve'].split(' ')[0].split('_')[0])\n",
    "            cell_type = row['systematicType']\n",
    "            if isinstance(row['type'], str) and row['type'] != row['systematicType']:\n",
    "                info['synonym'] = row['type']\n",
    "        elif isinstance(row['type'], str):\n",
    "            if isinstance(row['somaNeuromere'], str):\n",
    "                name = '{}_{}'.format(row['type'], row['somaNeuromere'])\n",
    "            elif isinstance(row['entryNerve'], str):\n",
    "                name = '{}_{}'.format(row['type'], row['entryNerve'].split('_')[0])\n",
    "            elif isinstance(row['exitNerve'], str):\n",
    "                name = '{}_{}'.format(row['type'], row['exitNerve'].split(' ')[0].split('_')[0])\n",
    "            cell_type = row['type']\n",
    "        else:\n",
    "            cell_type = 'unknown'\n",
    "            name = 'unknown_{}'.format(bodyID)\n",
    "        if isinstance(row['somaSide'], str):\n",
    "            name = '{}_{}'.format(name, row['somaSide'][0])\n",
    "    else:\n",
    "        name = row['instance']\n",
    "        if isinstance(row['systematicType'], str):\n",
    "            cell_type = row['systematicType']\n",
    "        elif isinstance(row['type'], str):\n",
    "            cell_type = row['type']\n",
    "        else:\n",
    "            cell_type = 'unknown'\n",
    "    if cell_type != 'unknown':\n",
    "        if name not in uname_dict:\n",
    "            uname_dict[name] = 0\n",
    "        uname_dict[name] += 1\n",
    "        name = '{}_{}'.format(name, uname_dict[name])\n",
    "    \n",
    "    added += 1    \n",
    "    c_neuropils = row['neuropils']\n",
    "    arborization = []\n",
    "    \n",
    "    if isinstance(c_neuropils, str):\n",
    "        dendrites = {j.split(':')[0]: int(j.split(':')[2]) for j in c_neuropils.split(';') if int(j.split(':')[2]) > 0}\n",
    "        axons = {j.split(':')[0]: int(j.split(':')[1]) for j in c_neuropils.split(';') if int(j.split(':')[1]) > 0}\n",
    "        arborization.append({'dendrites': dendrites, 'axons': axons, 'type': 'neuropil'})\n",
    "    \n",
    "    try:\n",
    "        df = na.load_swc('{}/{}.swc'.format(swc_dir, bodyID))\n",
    "        morphology = {'x': (df['x']*0.008).tolist(),\n",
    "                      'y': (df['y']*0.008).tolist(),\n",
    "                      'z': (df['z']*0.008).tolist(),\n",
    "                      'r': (df['r']*0.008).tolist(),\n",
    "                      'parent': df['parent'].tolist(),\n",
    "                      'identifier': [0]*(len(df['x'])),\n",
    "                      'sample': df['sample'].tolist(),\n",
    "                      'type': 'swc'}\n",
    "    except FileNotFoundError:\n",
    "        morphology = None\n",
    "        if segment: # no name, not traced, no morph\n",
    "            to_combine.append(bodyID)\n",
    "            continue\n",
    "        else:\n",
    "            segment = True\n",
    "            loaded_to_segment.append(bodyID)\n",
    "    \n",
    "    if isinstance(row['statusLabel'], str):\n",
    "        info['MANC Trace Status'] = row['statusLabel']\n",
    "    else:\n",
    "        info['MANC Trace Status'] = 'Untraced'\n",
    "\n",
    "    if not segment:\n",
    "        if isinstance(row['predictedNt'], str):\n",
    "            if row['predictedNt'] != 'unknown':\n",
    "                neurotransmitter = [neurotransmitter_translation[row['predictedNt']]]\n",
    "                info['predicted_neurotransmitter_probability'] = str(row['predictedNtProb'])\n",
    "        if isinstance(row['somaLocation'], str):\n",
    "            soma_loc = {k:v*0.008 for k,v in eval(row[\"somaLocation\"]).items()}\n",
    "            info['soma_location'] = \"{},{},{}\".format(soma_loc['x'],soma_loc['y'],soma_loc['z'])\n",
    "        if isinstance(row['hemilineage'], str):\n",
    "            info['hemilineage'] = row['hemilineage']\n",
    "        if isinstance(row['class'], str):\n",
    "            info['class'] = row['class']\n",
    "        if isinstance(row['subclass'], str):\n",
    "            info['subclass'] = row['subclass']\n",
    "        if isinstance(row['modality'], str):\n",
    "            info['modality'] = row['modality']\n",
    "        \n",
    "        vnc.add_Neuron(name, # uname\n",
    "                       cell_type, # name\n",
    "                       referenceId = str(bodyID), #referenceId\n",
    "                       info = info if len(info) else None,\n",
    "                       morphology = morphology,\n",
    "                       arborization = arborization,\n",
    "                       neurotransmitters = neurotransmitter)\n",
    "    else:\n",
    "        cell_type = 'segment'\n",
    "        name = 'segment_{}'.format(bodyID)\n",
    "        vnc.add_NeuronFragment(name,\n",
    "                               cell_type,\n",
    "                               referenceId = str(bodyID),\n",
    "                               info = info if len(info) else None,\n",
    "                               morphology = morphology,\n",
    "                               arborization = arborization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaaa7fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "not_chosen = neuron_list[~choose]\n",
    "n_added = 0\n",
    "for i, row in tqdm(not_chosen.iterrows()):\n",
    "    bodyID = row['bodyID']\n",
    "\n",
    "    if not os.path.exists('{}/{}.swc'.format(swc_dir, bodyID)):\n",
    "        continue\n",
    "\n",
    "    cell_type = 'segment'\n",
    "    name = 'segment_{}'.format(bodyID)\n",
    "\n",
    "    info = {}\n",
    "\n",
    "    added += 1\n",
    "\n",
    "    c_neuropils = row['neuropils']\n",
    "    arborization = []\n",
    "    if isinstance(c_neuropils, str):\n",
    "        dendrites = {j.split(':')[0]: int(j.split(':')[2]) for j in c_neuropils.split(';') if int(j.split(':')[2]) > 0}\n",
    "        axons = {j.split(':')[0]: int(j.split(':')[1]) for j in c_neuropils.split(';') if int(j.split(':')[1]) > 0}\n",
    "        arborization.append({'dendrites': dendrites, 'axons': axons, 'type': 'neuropil'})\n",
    "    \n",
    "    df = na.load_swc('{}/{}.swc'.format(swc_dir, bodyID))\n",
    "    morphology = {'x': (df['x']*0.008).tolist(),\n",
    "                  'y': (df['y']*0.008).tolist(),\n",
    "                  'z': (df['z']*0.008).tolist(),\n",
    "                  'r': (df['r']*0.008).tolist(),\n",
    "                  'parent': df['parent'].tolist(),\n",
    "                  'identifier': [0]*(len(df['x'])),\n",
    "                  'sample': df['sample'].tolist(),\n",
    "                  'type': 'swc'}\n",
    "    if isinstance(row['statusLabel'], str):\n",
    "        info['MANC Trace Status'] = row['statusLabel']\n",
    "    else:\n",
    "        info['MANC Trace Status'] = 'Untraced'\n",
    "\n",
    "    vnc.add_NeuronFragment(name,\n",
    "                                 cell_type,\n",
    "                                 referenceId = str(bodyID),\n",
    "                                 info = info if len(info) else None,\n",
    "                                 morphology = morphology,\n",
    "                                 arborization = arborization)\n",
    "    n_added += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860a54d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "vnc.flush_edges()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dcb102d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find all the neurons so they can be keyed by their referenceId.\n",
    "neurons = vnc.sql_query('select from NeuronAndFragment').node_objs\n",
    "# set the cache so there is no need for database access.\n",
    "for neuron in neurons:\n",
    "    vnc.set('NeuronAndFragment', neuron.uname, neuron, vnc.default_DataSource)\n",
    "neuron_ref_to_obj = {int(neuron.referenceId): neuron for neuron in neurons}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009b8da7-f767-451f-9184-31fb7e0c9365",
   "metadata": {},
   "source": [
    "### Load Synapses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158dba0e-7c44-4ab2-a1e3-ec385a7cd3ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tmp = {}\n",
    "morph_dict = {}\n",
    "\n",
    "for chunk in tqdm(pd.read_csv('synapses_all.csv', chunksize=100000)):\n",
    "    for i, row in chunk.iterrows():\n",
    "        pre = int(row['pre_id'])\n",
    "        post = int(row['post_id'])\n",
    "        if pre not in neuron_ref_to_obj:\n",
    "            pre = -1\n",
    "        if post not in neuron_ref_to_obj:\n",
    "            post = -1\n",
    "        if pre == -1 and post == -1:\n",
    "            continue\n",
    "        \n",
    "        pre_conf = np.array(eval(row['pre_confidence']))/1e6\n",
    "        post_conf = np.array(eval(row['post_confidence']))/1e6\n",
    "        NHP = np.sum(np.logical_and(post_conf>=0.7, pre_conf>=0.7))\n",
    "        \n",
    "        if pre == -1:\n",
    "            if post not in tmp:\n",
    "                tmp[post] = {}\n",
    "            if 'pre' not in tmp[post]:\n",
    "                tmp[post]['pre'] = {'pre_x': [], 'pre_y': [], 'pre_z': [], 'post_x': [], 'post_y': [], 'post_z': [],\n",
    "                                    'pre_confidence': [], 'post_confidence': [],\n",
    "                                    'N': 0, 'NHP': 0}\n",
    "            tmp[post]['pre']['pre_x'].append(np.array(eval(row['pre_x']))*0.008)\n",
    "            tmp[post]['pre']['pre_y'].append(np.array(eval(row['pre_y']))*0.008)\n",
    "            tmp[post]['pre']['pre_z'].append(np.array(eval(row['pre_z']))*0.008)\n",
    "            tmp[post]['pre']['post_x'].append(np.array(eval(row['post_x']))*0.008)\n",
    "            tmp[post]['pre']['post_y'].append(np.array(eval(row['post_y']))*0.008)\n",
    "            tmp[post]['pre']['post_z'].append(np.array(eval(row['post_z']))*0.008)\n",
    "            tmp[post]['pre']['pre_confidence'].append(pre_conf)\n",
    "            tmp[post]['pre']['post_confidence'].append(post_conf)\n",
    "            tmp[post]['pre']['N'] += row['N']\n",
    "            tmp[post]['pre']['NHP'] += NHP                        \n",
    "        elif post == -1:\n",
    "            if pre not in tmp:\n",
    "                tmp[pre] = {}\n",
    "            if 'post' not in tmp[pre]:\n",
    "                tmp[pre]['post'] = {'pre_x': [], 'pre_y': [], 'pre_z': [], 'post_x': [], 'post_y': [], 'post_z': [],\n",
    "                                    'pre_confidence': [], 'post_confidence': [],\n",
    "                                    'N': 0, 'NHP': 0}\n",
    "            tmp[pre]['post']['pre_x'].append(np.array(eval(row['pre_x']))*0.008)\n",
    "            tmp[pre]['post']['pre_y'].append(np.array(eval(row['pre_y']))*0.008)\n",
    "            tmp[pre]['post']['pre_z'].append(np.array(eval(row['pre_z']))*0.008)\n",
    "            tmp[pre]['post']['post_x'].append(np.array(eval(row['post_x']))*0.008)\n",
    "            tmp[pre]['post']['post_y'].append(np.array(eval(row['post_y']))*0.008)\n",
    "            tmp[pre]['post']['post_z'].append(np.array(eval(row['post_z']))*0.008)\n",
    "            tmp[pre]['post']['pre_confidence'].append(pre_conf)\n",
    "            tmp[pre]['post']['post_confidence'].append(post_conf)\n",
    "            tmp[pre]['post']['N'] += row['N']\n",
    "            tmp[pre]['post']['NHP'] += NHP\n",
    "        else:\n",
    "            pre_neuron = neuron_ref_to_obj[pre]\n",
    "            post_neuron = neuron_ref_to_obj[post]\n",
    "            c_neuropils = row['neuropils']\n",
    "            arborization = []\n",
    "            neuropils = {}\n",
    "            if isinstance(c_neuropils, str):\n",
    "                arborization.append({'type': 'neuropil',\n",
    "                               'synapses': {j.split(':')[0]: int(j.split(':')[1]) \\\n",
    "                                            for j in c_neuropils.split(';') \\\n",
    "                                            if int(j.split(':')[1]) > 0}})\n",
    "            content = {'type': 'swc'}\n",
    "            content['x'] = (np.array(eval(row['pre_x'])+eval(row['post_x']))*0.008).tolist()\n",
    "            content['y'] = (np.array(eval(row['pre_y'])+eval(row['post_y']))*0.008).tolist()\n",
    "            content['z'] = (np.array(eval(row['pre_z'])+eval(row['post_z']))*0.008).tolist()\n",
    "            content['r'] = [0]*len(content['x'])\n",
    "            content['parent'] = [-1]*(len(content['x'])//2) + [i+1 for i in range(len(content['x'])//2)]\n",
    "            content['identifier'] = [7]*(len(content['x'])//2) + [8]*(len(content['x'])//2)\n",
    "            content['sample'] = [i+1 for i in range(len(content['x']))]\n",
    "            content['confidence'] = pre_conf.tolist() + post_conf.tolist()\n",
    "\n",
    "            synapse = vnc.add_Synapse(pre_neuron, post_neuron, N = row['N'], NHP = NHP)\n",
    "            morph_dict[synapse._id] = {'morphology': content,\n",
    "                                       'arborization': arborization}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a3cc36-253e-4641-8d97-8ccba1a76c60",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vnc.flush_edges()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514799bd-a9a9-4616-b895-ce860d5378ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "version = '1.0'\n",
    "species = vnc.sql_query('select from Species').node_objs[0]\n",
    "notional_datasource = vnc.add_DataSource('notional', version = version,\n",
    "                                         species = species)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4b7774-4cac-4f4b-9b75-6e798eaa9b09",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for n, v in tqdm(tmp.items()):\n",
    "    if 'pre' in v:\n",
    "        post_neuron = neuron_ref_to_obj[n]\n",
    "        cell_type = 'combined_untraced_segments'\n",
    "        name = 'segments_presynaptic_to_{}'.format(post_neuron.uname)\n",
    "        pre_neuron = vnc.add_NeuronFragment(\n",
    "                                     name,\n",
    "                                     cell_type,\n",
    "                                     data_source = notional_datasource)\n",
    "        \n",
    "        content = {'type': 'swc'}\n",
    "        content['x'] = np.concatenate(v['pre']['pre_x'] + v['pre']['post_x']).tolist()\n",
    "        content['y'] = np.concatenate(v['pre']['pre_y'] + v['pre']['post_y']).tolist()\n",
    "        content['z'] = np.concatenate(v['pre']['pre_z'] + v['pre']['post_z']).tolist()\n",
    "        content['r'] = [0]*len(content['x'])\n",
    "        content['parent'] = [-1]*(len(content['x'])//2) + [i+1 for i in range(len(content['x'])//2)]\n",
    "        content['identifier'] = [7]*(len(content['x'])//2) + [8]*(len(content['x'])//2)\n",
    "        content['sample'] = [i+1 for i in range(len(content['x']))]\n",
    "        content['confidence'] = np.concatenate(v['pre']['pre_confidence'] + v['pre']['post_confidence']).tolist()\n",
    "        synapse = vnc.add_Synapse(pre_neuron, post_neuron, N = v['pre']['N'], NHP = v['pre']['NHP'])\n",
    "        morph_dict[synapse._id] = {'morphology': content}\n",
    "    if 'post' in v:\n",
    "        pre_neuron = neuron_ref_to_obj[n]\n",
    "        cell_type = 'combined_untraced_segments'\n",
    "        name = 'segments_postsynaptic_to_{}'.format(pre_neuron.uname)\n",
    "        post_neuron = vnc.add_NeuronFragment(\n",
    "                                     name,\n",
    "                                     cell_type,\n",
    "                                     data_source = notional_datasource)\n",
    "        content = {'type': 'swc'}\n",
    "        content['x'] = np.concatenate(v['post']['pre_x'] + v['post']['post_x']).tolist()\n",
    "        content['y'] = np.concatenate(v['post']['pre_y'] + v['post']['post_y']).tolist()\n",
    "        content['z'] = np.concatenate(v['post']['pre_z'] + v['post']['post_z']).tolist()\n",
    "        content['r'] = [0]*len(content['x'])\n",
    "        content['parent'] = [-1]*(len(content['x'])//2) + [i+1 for i in range(len(content['x'])//2)]\n",
    "        content['identifier'] = [7]*(len(content['x'])//2) + [8]*(len(content['x'])//2)\n",
    "        content['sample'] = [i+1 for i in range(len(content['x']))]\n",
    "        content['confidence'] = np.concatenate(v['post']['pre_confidence'] + v['post']['post_confidence']).tolist()\n",
    "        synapse = vnc.add_Synapse(pre_neuron, post_neuron, N = v['post']['N'], NHP = v['post']['NHP'])\n",
    "        morph_dict[synapse._id] = {'morphology': content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbdc15a-df7a-4205-9134-07e2ceceaa66",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vnc.flush_edges()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4fa60ba-709d-4269-b84e-1cd35a17d205",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for rid, data in tqdm(morph_dict.items()):\n",
    "    if 'morphology' in data:\n",
    "        vnc.add_morphology(rid, data['morphology'])\n",
    "    if 'arborization' in data:\n",
    "        vnc.add_synapse_arborization(rid, data['arborization'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191d688e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
